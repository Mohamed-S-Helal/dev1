{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic-OCR2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzNHEgwMQ72Bb/mXzMvFzf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed-S-Helal/dev1/blob/main/Arabic_OCR2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "ZeZLDynCygSS"
      },
      "outputs": [],
      "source": [
        "# !wget -q https://raw.githubusercontent.com/Mohamed-S-Helal/Arabic-OCR/master/requirements.txt -O /content/requirements.txt\n",
        "# !pip install -r requirements.txt -q\n",
        "# !wget -q https://github.com/Mohamed-S-Helal/Arabic-OCR/archive/refs/heads/master.zip -O /content/master.zip\n",
        "# !unzip -qnd /content/ /content/master.zip "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import pickle\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import re\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "from skimage.morphology import skeletonize, thin\n",
        "from scipy.ndimage import interpolation as inter\n",
        "from PIL import Image as im"
      ],
      "metadata": {
        "id": "KX0UDWmKzVmT"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = '2L_NN.sav'\n",
        "def load_model():\n",
        "    location = '/content/Arabic-OCR-master/src/models'\n",
        "    if os.path.exists(location):\n",
        "        model = pickle.load(open(f'/content/Arabic-OCR-master/src/models/{model_name}', 'rb'))\n",
        "        return model"
      ],
      "metadata": {
        "id": "LlcVlHFO3Wdy"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from importlib.util import module_from_spec, spec_from_file_location\n",
        "# def load_module(path, name):\n",
        "#   spec = spec_from_file_location(name, os.path.join(path, name+\".py\"))\n",
        "#   module = module_from_spec(spec)\n",
        "#   spec.loader.exec_module(module_from_spec(spec))\n",
        "#   return module"
      ],
      "metadata": {
        "id": "1So5h0OJ4f9-"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from character_segmentation import segment\n",
        "# from segmentation import extract_words\n",
        "# from train import prepare_char, featurizer\n",
        "# from preprocessing import binary_otsus, deskew\n",
        "# from utilities import projection, save_image"
      ],
      "metadata": {
        "id": "gJWJugBZ_E37"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modules_path = r'/content/Arabic-OCR-master/src/'\n",
        "# character_segmentation = load_module(modules_path,'character_segmentation')\n",
        "# segmentation = load_module(modules_path,'segmentation')\n",
        "# train = load_module(modules_path,'train')"
      ],
      "metadata": {
        "id": "B5bGUwhV9oW8"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PDyHDPfhBek6"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_otsus(image, filter:int=1):\n",
        "    \"\"\"Binarize an image 0's and 255's using Otsu's Binarization\"\"\"\n",
        "\n",
        "    if len(image.shape) == 3:\n",
        "        gray_img = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        gray_img = image\n",
        "\n",
        "    # Otsus Binarization\n",
        "    if filter != 0:\n",
        "        blur = cv.GaussianBlur(gray_img, (3,3), 0)\n",
        "        binary_img = cv.threshold(blur, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)[1]\n",
        "    else:\n",
        "        binary_img = cv.threshold(gray_img, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)[1]\n",
        "    \n",
        "    # Morphological Opening\n",
        "    # kernel = np.ones((3,3),np.uint8)\n",
        "    # clean_img = cv.morphologyEx(binary_img, cv.MORPH_OPEN, kernel)\n",
        "\n",
        "    return binary_img\n",
        "\n",
        "\n",
        "def find_score(arr, angle):\n",
        "    data = inter.rotate(arr, angle, reshape=False, order=0)\n",
        "    hist = np.sum(data, axis=1)\n",
        "    score = np.sum((hist[1:] - hist[:-1]) ** 2)\n",
        "    return hist, score\n",
        "\n",
        "def deskew(binary_img):\n",
        "\n",
        "    \n",
        "    ht, wd = binary_img.shape\n",
        "    # _, binary_img = cv.threshold(img, 127, 255, cv.THRESH_BINARY)\n",
        "\n",
        "    # pix = np.array(img.convert('1').getdata(), np.uint8)\n",
        "    bin_img = (binary_img // 255.0)\n",
        "\n",
        "    delta = 0.1\n",
        "    limit = 3\n",
        "    angles = np.arange(-limit, limit+delta, delta)\n",
        "    scores = []\n",
        "    for angle in angles:\n",
        "        hist, score = find_score(bin_img, angle)\n",
        "        scores.append(score)\n",
        "\n",
        "    best_score = max(scores)\n",
        "    best_angle = angles[scores.index(best_score)]\n",
        "    # print('Best angle: {}'.formate(best_angle))\n",
        "\n",
        "    # correct skew\n",
        "    data = inter.rotate(bin_img, best_angle, reshape=False, order=0)\n",
        "    img = im.fromarray((255 * data).astype(\"uint8\"))\n",
        "\n",
        "    # img.save('skew_corrected.png')\n",
        "    pix = np.array(img)\n",
        "    return pix\n",
        "\n",
        "def vexpand(gray_img, color:int):\n",
        "    \"\"\"Expand the image by some space vertically in both directions\"\"\"\n",
        "\n",
        "    color = 1 if color > 0 else 0\n",
        "    (h, w) = gray_img.shape[:2]\n",
        "    space = np.ones((10, w)) * 255 * color\n",
        "\n",
        "    return np.block([[space], [gray_img], [space]])\n",
        "\n",
        "\n",
        "def hexpand(gray_img, color:int):\n",
        "    \"\"\"Expand the image by some space horizontally in both directions\"\"\"\n",
        "\n",
        "    color = 1 if color > 0 else 0\n",
        "    (h, w) = gray_img.shape[:2]\n",
        "    space = np.ones((h, 10)) * 255 * color\n",
        "\n",
        "    return np.block([space, gray_img, space])\n",
        "\n",
        "def valid(row, col, vis, word):\n",
        "    return (row < vis.shape[0] and col < vis.shape[1] and row >= 0 and col >=0 and vis[row][col] == 0 and word[row][col] > 0)\n",
        "\n",
        "def dfs(row, col, vis, word):\n",
        "\n",
        "    dX = [0,0,1,1,-1,-1,1,-1]\n",
        "    dY = [1,-1,0,1,0,-1,-1,1]\n",
        "    vis[row][col] += 1\n",
        "    for i in range(8):\n",
        "        if(valid(row+dX[i],col+dY[i],vis, word)):\n",
        "            dfs(row+dX[i], col+dY[i], vis, word)\n",
        "    return\n"
      ],
      "metadata": {
        "id": "d8F9WCudC9D0"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#utilities\n",
        "\n",
        "def save_image(img, folder, title):\n",
        "    cv.imwrite(f'./{folder}/{title}.png', img)\n",
        "\n",
        "\n",
        "def projection(gray_img, axis:str='horizontal'):\n",
        "    \"\"\" Compute the horizontal or the vertical projection of a gray image \"\"\"\n",
        "\n",
        "    if axis == 'horizontal':\n",
        "        projection_bins = np.sum(gray_img, 1).astype('int32')\n",
        "    elif axis == 'vertical':\n",
        "        projection_bins = np.sum(gray_img, 0).astype('int32')\n",
        "\n",
        "    return projection_bins"
      ],
      "metadata": {
        "id": "cgwTzOBtBbXb"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#segmentation\n",
        "\n",
        "def preprocess(image):\n",
        "\n",
        "    # Maybe we end up using only gray level image.\n",
        "    gray_img = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "    gray_img = cv.bitwise_not(gray_img)\n",
        "\n",
        "    binary_img = binary_otsus(gray_img, 0)\n",
        "    # cv.imwrite('origin.png', gray_img)\n",
        "\n",
        "    # deskewed_img = deskew(binary_img)\n",
        "    deskewed_img = deskew(binary_img)\n",
        "    # cv.imwrite('output.png', deskewed_img)\n",
        "\n",
        "    # binary_img = binary_otsus(deskewed_img, 0)\n",
        "    # breakpoint()\n",
        "\n",
        "    # Visualize\n",
        "\n",
        "    # breakpoint()\n",
        "    return deskewed_img\n",
        "\n",
        "\n",
        "def projection_segmentation(clean_img, axis, cut=3):\n",
        "    \n",
        "    segments = []\n",
        "    start = -1\n",
        "    cnt = 0\n",
        "\n",
        "    projection_bins = projection(clean_img, axis)\n",
        "    for idx, projection_bin in enumerate(projection_bins):\n",
        "\n",
        "        if projection_bin != 0:\n",
        "            cnt = 0\n",
        "        if projection_bin != 0 and start == -1:\n",
        "            start = idx\n",
        "        if projection_bin == 0 and start != -1:\n",
        "            cnt += 1\n",
        "            if cnt >= cut:\n",
        "                if axis == 'horizontal':\n",
        "                    segments.append(clean_img[max(start-1, 0):idx, :])\n",
        "                elif axis == 'vertical':\n",
        "                    segments.append(clean_img[:, max(start-1, 0):idx])\n",
        "                cnt = 0\n",
        "                start = -1\n",
        "    \n",
        "    return segments\n",
        "\n",
        "\n",
        "# Line Segmentation\n",
        "#----------------------------------------------------------------------------------------\n",
        "def line_horizontal_projection(image, cut=3):\n",
        "\n",
        "    # Preprocess input image\n",
        "    clean_img = preprocess(image)\n",
        "\n",
        "\n",
        "    # Segmentation    \n",
        "    lines = projection_segmentation(clean_img, axis='horizontal', cut=cut)\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "# Word Segmentation\n",
        "#----------------------------------------------------------------------------------------\n",
        "def word_vertical_projection(line_image, cut=3):\n",
        "    \n",
        "    line_words = projection_segmentation(line_image, axis='vertical', cut=cut)\n",
        "    line_words.reverse()\n",
        "    \n",
        "    return line_words\n",
        "\n",
        "\n",
        "def extract_words(img, visual=0):\n",
        "\n",
        "    lines = line_horizontal_projection(img)\n",
        "    words = []\n",
        "    \n",
        "    for idx, line in enumerate(lines):\n",
        "        \n",
        "        if visual:\n",
        "            save_image(line, 'lines', f'line{idx}')\n",
        "\n",
        "        line_words = word_vertical_projection(line)\n",
        "        for w in line_words:\n",
        "            # if len(words) == 585:\n",
        "            #     print(idx)\n",
        "            words.append((w, line))\n",
        "        # words.extend(line_words)\n",
        "\n",
        "    # breakpoint()\n",
        "    if visual:\n",
        "        for idx, word in enumerate(words):\n",
        "            save_image(word[0], 'words', f'word{idx}')\n",
        "\n",
        "    return words\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "    \n",
        "#     img = cv.imread('../Dataset/scanned/capr196.png')\n",
        "#     extract_words(img, 1)"
      ],
      "metadata": {
        "id": "bm1S5dorBk8l"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#character segmentation\n",
        "\n",
        "def binarize(word_img):\n",
        "\n",
        "    _, binary_img = cv.threshold(word_img, 127, 255, cv.THRESH_BINARY)\n",
        "    # _, binary_img = cv.threshold(word_img, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "\n",
        "    return binary_img // 255\n",
        "\n",
        "\n",
        "def fill(binary_img, VP):\n",
        "\n",
        "    (h, w) = binary_img.shape\n",
        "\n",
        "    flag = 1\n",
        "    while flag:\n",
        "        flag = 0\n",
        "        for row in range(h-1):\n",
        "            for col in range(1, w-1):\n",
        "\n",
        "                if binary_img[row][col] == 0 and binary_img[row][col-1] == 1 and binary_img[row][col+1] == 1 and binary_img[row+1][col] == 1 and VP[col] != 0:\n",
        "                    binary_img[row][col] = 1\n",
        "                    # flag = 1\n",
        "\n",
        "    return binary_img\n",
        "    \n",
        "\n",
        "def baseline_detection(word_img):\n",
        "    '''Get baseline index of a given word'''\n",
        "\n",
        "    HP = projection(word_img, 'horizontal')\n",
        "    peak = np.amax(HP)\n",
        "\n",
        "    # Array of indices of max element\n",
        "    baseline_idx = np.where(HP == peak)[0]\n",
        "\n",
        "    # Get first or last index\n",
        "    upper_base = baseline_idx[0]\n",
        "    lower_base = baseline_idx[-1]\n",
        "    thickness = abs(lower_base - upper_base) + 1\n",
        "    \n",
        "    return upper_base, lower_base, thickness\n",
        "\n",
        "\n",
        "def horizontal_transitions(word_img, baseline_idx):\n",
        "    \n",
        "    max_transitions = 0\n",
        "    max_transitions_idx = baseline_idx\n",
        "    line_idx = baseline_idx-1\n",
        "    lines = []\n",
        "    # new temp image with no dots above baseline\n",
        "    \n",
        "    while line_idx >= 0:\n",
        "        current_transitions = 0\n",
        "        flag = 0\n",
        "\n",
        "        horizontal_line = word_img[line_idx, :]\n",
        "        for pixel in reversed(horizontal_line):\n",
        "\n",
        "            if pixel == 1 and flag == 0:\n",
        "                current_transitions += 1\n",
        "                flag = 1\n",
        "            elif pixel == 0 and flag == 1:\n",
        "                current_transitions += 1\n",
        "                flag = 0\n",
        "                \n",
        "        if current_transitions >= max_transitions:\n",
        "            max_transitions = current_transitions\n",
        "            lines.append(line_idx)\n",
        "            max_transitions_idx = line_idx\n",
        "\n",
        "        line_idx -= 1\n",
        "    \n",
        "    return lines[len(lines)//2]\n",
        "\n",
        "\n",
        "def vertical_transitions(word_img, cut):\n",
        "    \n",
        "    transitions = 0\n",
        "\n",
        "    vertical_line = word_img[:, cut]\n",
        "\n",
        "    flag = 0\n",
        "    for pixel in vertical_line:\n",
        "\n",
        "        if pixel == 1 and flag == 0:\n",
        "            transitions += 1\n",
        "            flag = 1\n",
        "        elif pixel == 0 and flag == 1:\n",
        "            transitions += 1\n",
        "            flag = 0\n",
        "\n",
        "    return transitions\n",
        "\n",
        "\n",
        "def cut_points(word_img, VP, MFV, MTI, baseline_idx):\n",
        "      \n",
        "    # flag to know the start of the word\n",
        "    f = 0\n",
        "\n",
        "    flag = 0\n",
        "    (h, w) = word_img.shape\n",
        "    i = w-1\n",
        "    separation_regions = []\n",
        "\n",
        "    wrong = 0\n",
        "    # loop over the width of the image from right to left\n",
        "    while i >= 0:\n",
        "\n",
        "        pixel = word_img[MTI, i]\n",
        "        \n",
        "        if pixel == 1 and f == 0:\n",
        "            f = 1\n",
        "            flag = 1\n",
        "\n",
        "        if f == 1:\n",
        "\n",
        "            # Get start and end of separation region (both are black pixels <----)\n",
        "            if pixel == 0 and flag == 1:\n",
        "                start = i+1\n",
        "                flag = 0\n",
        "            elif pixel == 1 and flag == 0:\n",
        "                end = i         # end maybe = i not i+1\n",
        "                flag = 1\n",
        "\n",
        "                mid = (start + end) // 2\n",
        "\n",
        "                left_zero = -1\n",
        "                left_MFV = -1\n",
        "                right_zero = -1\n",
        "                right_MFV = -1\n",
        "                # threshold for MFV\n",
        "                T = 1\n",
        "\n",
        "                j = mid - 1\n",
        "                # loop from mid to end to get nearest VP = 0 and VP = MFV\n",
        "                while j >= end:\n",
        "                    \n",
        "                    if VP[j] == 0 and left_zero == -1:\n",
        "                        left_zero = j\n",
        "                    if VP[j] <= MFV + T and left_MFV == -1:\n",
        "                        left_MFV = j\n",
        "\n",
        "                    # if left_zero != -1 and left_MFV != -1:\n",
        "                    #     break\n",
        "\n",
        "                    j -= 1\n",
        "\n",
        "                j = mid\n",
        "                # loop from mid to start to get nearest VP = 0 and VP = MFV\n",
        "                while j <= start:\n",
        "\n",
        "                    if VP[j] == 0 and right_zero == -1:\n",
        "                        right_zero = j\n",
        "                    if VP[j] <= MFV + T and right_MFV == -1:\n",
        "                        right_MFV = j\n",
        "\n",
        "                    if right_zero != -1 and right_MFV != -1:\n",
        "                        break\n",
        "\n",
        "                    j += 1\n",
        "\n",
        "                # Check for VP = 0 first\n",
        "                if VP[mid] == 0:\n",
        "                    cut_index = mid\n",
        "                elif left_zero != -1 and right_zero != -1:\n",
        "                    \n",
        "                    if abs(left_zero-mid) <= abs(right_zero-mid):\n",
        "                        cut_index = left_zero\n",
        "                    else:\n",
        "                        cut_index = right_zero\n",
        "                elif left_zero != -1:\n",
        "                    cut_index = left_zero\n",
        "                elif right_zero != -1:\n",
        "                    cut_index = right_zero\n",
        "\n",
        "                # Check for VP = MFV second\n",
        "                # elif VP[mid] <= MFV+T:\n",
        "                #     cut_index = mid\n",
        "                elif left_MFV != -1:\n",
        "                    cut_index = left_MFV\n",
        "                elif right_MFV != -1:\n",
        "                    cut_index = right_MFV\n",
        "                else:\n",
        "                    cut_index = mid\n",
        "\n",
        "\n",
        "                seg = word_img[:, end:start]\n",
        "                HP = projection(seg, 'horizontal')\n",
        "                SHPA = np.sum(HP[:MTI])\n",
        "                SHPB = np.sum(HP[MTI+1:])\n",
        "                \n",
        "                top = 0\n",
        "                for idx, proj in enumerate(HP):\n",
        "                    if proj != 0:\n",
        "                        top = idx\n",
        "                        break\n",
        "\n",
        "                cnt = 0\n",
        "                for k in range(end, cut_index+1):\n",
        "                    if vertical_transitions(word_img, k) > 2:\n",
        "                        cnt = 1\n",
        "                if SHPB == 0 and (baseline_idx - top) <= 5 and cnt == 1:\n",
        "                    # breakpoint()\n",
        "                    wrong = 1\n",
        "                else:\n",
        "                    separation_regions.append((end, cut_index, start))\n",
        "\n",
        "        i -= 1\n",
        "\n",
        "    return separation_regions, wrong\n",
        "\n",
        "\n",
        "def check_baseline(word_img, start, end, upper_base, lower_base):\n",
        "    \n",
        "    j = end+1\n",
        "\n",
        "    cnt = 0\n",
        "    while j < start:\n",
        "    \n",
        "        # Black pixel (Discontinuity)\n",
        "        base = upper_base\n",
        "        while base <= lower_base:\n",
        "            \n",
        "            pixel = word_img[base][j]\n",
        "            cnt += pixel\n",
        "\n",
        "            base += 1\n",
        "        \n",
        "        j += 1\n",
        "\n",
        "    if cnt == 0:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def inside_hole(word_img, end_idx, start_idx):\n",
        "    '''Check if a segment has a hole or not'''\n",
        "\n",
        "    if end_idx == 0 and start_idx == 0:\n",
        "        return 0\n",
        "\n",
        "    sk = skeletonize(word_img)\n",
        "    j = end_idx + 1\n",
        "    flag = 1\n",
        "    while j < start_idx:\n",
        "        VT = vertical_transitions(sk, j)\n",
        "        if VT <= 2:\n",
        "            flag = 0\n",
        "            break\n",
        "        j += 1\n",
        "    \n",
        "    return flag\n",
        "\n",
        "\n",
        "def check_hole(segment):\n",
        "    '''Check if a segment has a hole or not'''\n",
        "\n",
        "    # no_dots = segment.copy()\n",
        "\n",
        "    contours, hierarchy = cv.findContours(segment, cv.RETR_TREE, cv.CHAIN_APPROX_NONE)\n",
        "    cnt = 0\n",
        "    for hier in hierarchy[0]:\n",
        "        if hier[3] >= 0:\n",
        "            cnt += 1\n",
        "\n",
        "    return cnt != 0\n",
        "\n",
        "\n",
        "def remove_dots(word_img, threshold=11):\n",
        "\n",
        "    no_dots = word_img.copy()\n",
        "\n",
        "    components, labels, stats, GoCs = cv.connectedComponentsWithStats(no_dots, connectivity=8)\n",
        "    char = []\n",
        "    for label in range(1, components):\n",
        "        _, _, _, _, size = stats[label]\n",
        "        if size > threshold:\n",
        "            char.append(label)\n",
        "    for label in range(1, components):\n",
        "        _, _, _, _, size = stats[label]\n",
        "        if label not in  char:\n",
        "            no_dots[labels == label] = 0\n",
        "\n",
        "    return no_dots\n",
        "\n",
        "\n",
        "def check_dots(segment):\n",
        "\n",
        "    contours, hierarchy = cv.findContours(segment[:, 1:segment.shape[1]-1], cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    cnt = 0\n",
        "    for c in contours:\n",
        "        if len(c) >= 1:\n",
        "            cnt +=1 \n",
        "    return cnt > 1\n",
        "\n",
        "\n",
        "def check_stroke(no_dots_copy, segment, upper_base, lower_base, SR1, SR2):\n",
        "\n",
        "    T = 1\n",
        "    components, labels, stats, cen= cv.connectedComponentsWithStats(segment, connectivity=8)\n",
        "    skeleton = skeletonize(segment.copy()).astype(np.uint8)\n",
        "    (h, w) = segment.shape\n",
        "\n",
        "    cnt = 0\n",
        "    for label in range(1, components):\n",
        "        if stats[label][4] > 3:\n",
        "            cnt += 1\n",
        "        else:\n",
        "            segment[labels==label] = 0\n",
        "\n",
        "    if cnt > 2 or cnt == 0:\n",
        "        return False\n",
        "\n",
        "    if check_hole(segment) or inside_hole(no_dots_copy, SR1[0], SR1[1]) or inside_hole(no_dots_copy, SR2[0], SR2[1]):\n",
        "        return False\n",
        "\n",
        "    HP = projection(skeleton, 'horizontal')\n",
        "    VP = projection(segment, 'vertical')\n",
        "\n",
        "    seg_l = -1\n",
        "    seg_r = -1\n",
        "    for i in range(0, len(VP)):\n",
        "        if VP[i] != 0:\n",
        "            seg_l = i\n",
        "            break\n",
        "    for i in range(len(VP)-1, -1, -1):\n",
        "        if VP[i] != 0:\n",
        "            seg_r = i\n",
        "            break\n",
        "\n",
        "    seg_width = seg_r - seg_l + 1\n",
        "    SHPA = np.sum(HP[:upper_base])\n",
        "    SHPB = np.sum(HP[lower_base+T+1:])\n",
        "    MFV_HP = np.argmax(np.bincount(HP)[1:])+1\n",
        "    MFV = lower_base - upper_base + 1 + T\n",
        "\n",
        "    top_pixel = -1\n",
        "    for i, proj in enumerate(HP):\n",
        "        if proj != 0:\n",
        "            top_pixel = i\n",
        "            break\n",
        "    height = upper_base-top_pixel\n",
        "    \n",
        "    VT = 0\n",
        "    for i in range(w):\n",
        "        if vertical_transitions(skeleton, i) > 2:\n",
        "            VT += 1\n",
        "    cnt = 0\n",
        "    for proj in VP:\n",
        "        if proj >= height:\n",
        "            cnt += 2\n",
        "        elif proj == height-1:\n",
        "            cnt += 1\n",
        "    # abs(MFV - MFV_HP) <= 2\n",
        "    if SHPB == 0  and height <= 6 and VT <= 2 and seg_width <= 6 and cnt >= 2:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def filter_regions(word_img, no_dots_copy, SRL:list, VP:list, upper_base:int, lower_base:int, MTI:int, MFV:int, top_line:int):\n",
        "    \n",
        "    valid_separation_regions = []\n",
        "    overlap = []\n",
        "\n",
        "    T = 1\n",
        "    components, labels= cv.connectedComponents(word_img[:lower_base+5, :], connectivity=8)\n",
        "\n",
        "    SR_idx = 0\n",
        "    while SR_idx < len(SRL):\n",
        "        \n",
        "        SR = SRL[SR_idx]\n",
        "        end_idx, cut_idx, start_idx = SR\n",
        "\n",
        "        # Case 1 : Vertical Projection = 0\n",
        "        if VP[cut_idx] == 0:\n",
        "            valid_separation_regions.append(SR)\n",
        "            SR_idx += 1\n",
        "            continue\n",
        "\n",
        "      # Case 2 : no connected path between start and end\n",
        "        # components, labels= cv.connectedComponents(word_img[:, end_idx:start_idx+1], connectivity=8)\n",
        "        if labels[MTI, end_idx] != labels[MTI, start_idx]:\n",
        "            valid_separation_regions.append(SR)\n",
        "            overlap.append(SR)\n",
        "            SR_idx += 1\n",
        "            continue\n",
        "\n",
        "      \n",
        "\n",
        "        # Case 3 : Contain Holes\n",
        "        # if check_hole(no_dots_copy[:, end_idx: cut_idx]) and inside_hole(no_dots_copy, end_idx, start_idx):\n",
        "        cc, l = cv.connectedComponents(1-(no_dots_copy[:, end_idx:start_idx+1]), connectivity=4)\n",
        "        \n",
        "        if cc-1 >= 3 and inside_hole(no_dots_copy, end_idx, start_idx):\n",
        "            SR_idx += 1\n",
        "            continue\n",
        "       \n",
        "     \n",
        "        # Case 4 : No baseline between start and end\n",
        "        segment = no_dots_copy[:, end_idx+1: start_idx]\n",
        "        segment_width = start_idx-end_idx-1\n",
        "\n",
        "        j = end_idx+1\n",
        "        cnt = 0\n",
        "        while j < start_idx:\n",
        "            \n",
        "            # Black pixel (Discontinuity)\n",
        "            base = upper_base-T\n",
        "            while base <= lower_base+T:\n",
        "                \n",
        "                pixel = no_dots_copy[base][j]\n",
        "                cnt += pixel\n",
        "\n",
        "                base += 1\n",
        "            \n",
        "            j += 1\n",
        "\n",
        "        if cnt < segment_width-2 and segment_width > 4:\n",
        "            \n",
        "            segment_HP = projection(segment, 'horizontal')\n",
        "\n",
        "            SHPA = np.sum(segment_HP[:upper_base])\n",
        "            SHPB = np.sum(segment_HP[lower_base+T+1:])\n",
        "\n",
        "            if (int(SHPB) - int(SHPA)) >= 0:\n",
        "                SR_idx += 1\n",
        "                continue\n",
        "            elif VP[cut_idx] <= MFV + T:\n",
        "                valid_separation_regions.append(SR)\n",
        "                SR_idx += 1\n",
        "                continue\n",
        "            else:\n",
        "                SR_idx += 1\n",
        "                continue\n",
        "\n",
        "      \n",
        "        # if SR_idx == 0:\n",
        "        #     breakpoint()\n",
        "        # Case 5 : Last region or next VP[nextcut] = 0\n",
        "        if SR_idx == len(SRL) - 1 or VP[SRL[SR_idx+1][1]] == 0:\n",
        "\n",
        "            if SR_idx == len(SRL) - 1:\n",
        "                segment_dots = word_img[:, :SRL[SR_idx][1]+1]\n",
        "                segment = no_dots_copy[:, :SRL[SR_idx][1]+1]\n",
        "                next_cut = 0\n",
        "            else:\n",
        "                next_cut = SRL[SR_idx+1][1]\n",
        "                segment_dots = word_img[:, next_cut:SRL[SR_idx][1]+1]\n",
        "                segment = no_dots_copy[:, next_cut:SRL[SR_idx][1]+1]\n",
        "\n",
        "            segment_HP = projection(segment, 'horizontal')\n",
        "            (h, w) = segment.shape\n",
        "\n",
        "            top = -1\n",
        "            for i, proj in enumerate(segment_HP):\n",
        "                if proj != 0:\n",
        "                    top = i\n",
        "                    break\n",
        "            height = upper_base - top\n",
        "\n",
        "            # if SR_idx == len(SRL) - 1:\n",
        "                # breakpoint()\n",
        "            SHPA = np.sum(segment_HP[:upper_base])\n",
        "            SHPB = np.sum(segment_HP[lower_base+T+1:])\n",
        "            sk = skeletonize(segment).astype(np.uint8)\n",
        "            seg_VP = projection(segment, 'vertical')\n",
        "            non_zero =  np.nonzero(seg_VP)[0]\n",
        "            cnt = 0\n",
        "            # for k in range(0, (len(non_zero)//2)+(len(non_zero)%2)):\n",
        "            for k in range(0, 3):\n",
        "                if k >= len(non_zero):\n",
        "                    break\n",
        "                index = non_zero[k]\n",
        "                if seg_VP[index] >= height:\n",
        "                    cnt += 1\n",
        "            \n",
        "            if (SHPB <= 5 and cnt > 0 and height <= 6) or (len(non_zero) >= 10 and SHPB > SHPA and not check_dots(segment_dots)):\n",
        "                SR_idx += 1\n",
        "                continue\n",
        "                \n",
        "        # Strokes \n",
        "\n",
        "        SEGP = (-1, -1)\n",
        "        SEG = (-1, -1)\n",
        "        SEGN = (-1, -1)\n",
        "        SEGNN = (-1, -1)\n",
        "        SEGP_SR1 = (0, 0)\n",
        "        SEGP_SR2 = (0, 0)\n",
        "        SEG_SR1 = (0, 0)\n",
        "        SEG_SR2 = (0, 0)\n",
        "        SEGN_SR1 = (0, 0)\n",
        "        SEGN_SR2 = (0, 0)\n",
        "        SEGNN_SR1 = (0, 0)\n",
        "        SEGNN_SR2 = (0, 0)\n",
        "\n",
        "        current_cut = SR[1]\n",
        "     \n",
        "        if SR_idx == 0:\n",
        "            SEGP = (SRL[SR_idx][1], word_img.shape[1]-1)\n",
        "            SEGP_SR1 = (SRL[SR_idx][0], SRL[SR_idx][2])\n",
        "            SEGP_SR2 = (SRL[SR_idx][1], word_img.shape[1]-1)\n",
        "\n",
        "        if SR_idx > 0:\n",
        "            SEGP = (SRL[SR_idx][1], SRL[SR_idx-1][1])\n",
        "            SEGP_SR1 = (SRL[SR_idx][0], SRL[SR_idx][2])\n",
        "            SEGP_SR2 = (SRL[SR_idx-1][0], SRL[SR_idx-1][2])\n",
        "        \n",
        "        if SR_idx < len(SRL)-1:\n",
        "            SEG = (SRL[SR_idx+1][1], SRL[SR_idx][1])\n",
        "            SEG_SR1 = (SRL[SR_idx][0], SRL[SR_idx][2])\n",
        "            SEG_SR2 = (SRL[SR_idx+1][0], SRL[SR_idx+1][2])\n",
        "\n",
        "        if SR_idx < len(SRL)-2:\n",
        "            SEGN = (SRL[SR_idx+2][1], SRL[SR_idx+1][1])\n",
        "            SEGN_SR1 = (SRL[SR_idx+1][0], SRL[SR_idx+1][2])\n",
        "            SEGN_SR2 = (SRL[SR_idx+2][0], SRL[SR_idx+2][2])\n",
        "        elif SR_idx == len(SRL)-2:\n",
        "            SEGN = (0, SRL[SR_idx+1][1])\n",
        "            SEGN_SR1 = (SRL[SR_idx+1][0], SRL[SR_idx+1][2])\n",
        "            SEGN_SR2 = (0, SRL[SR_idx+1][2])\n",
        "\n",
        "            \n",
        "        if SR_idx < len(SRL)-3:\n",
        "            SEGNN = (SRL[SR_idx+3][1], SRL[SR_idx+2][1])\n",
        "            SEGNN_SR1 = (SRL[SR_idx+2][0], SRL[SR_idx+2][2])\n",
        "            SEGNN_SR2 = (SRL[SR_idx+3][0], SRL[SR_idx+3][2])\n",
        "\n",
        "            \n",
        "        # if SR_idx == 6:\n",
        "        #     breakpoint()\n",
        "        \n",
        "        # SEG is stroke with dots\n",
        "        if SEG[0] != -1 and\\\n",
        "            (check_stroke(no_dots_copy, no_dots_copy[:, SEG[0]:SEG[1]], upper_base, lower_base, SEG_SR1, SEG_SR2) \\\n",
        "            and check_dots(word_img[:, SEG[0]:SEG[1]])):\n",
        "            \n",
        "            # breakpoint()\n",
        "            # Case when starts with ش\n",
        "            if SEGP[0] != -1 and \\\n",
        "                ((check_stroke(no_dots_copy, no_dots_copy[:, SEGP[0]:SEGP[1]], upper_base, lower_base, SEGP_SR1, SEGP_SR2) \\\n",
        "                and not check_dots(word_img[:, SEGP[0]:SEGP[1]]))\\\n",
        "                and (SR_idx == 0 or VP[SRL[SR_idx-1][1]] == 0 or (VP[SRL[SR_idx-1][1]] == 0 and SRL[SR_idx-1] in overlap))):\n",
        "                \n",
        "                SR_idx += 2\n",
        "                continue\n",
        "            else:\n",
        "                valid_separation_regions.append(SR)\n",
        "                SR_idx += 1\n",
        "                continue\n",
        "                \n",
        "        # SEG is stroke without dots\n",
        "        elif SEG[0] != -1\\\n",
        "            and (check_stroke(no_dots_copy, no_dots_copy[:, SEG[0]:SEG[1]], upper_base, lower_base, SEG_SR1, SEG_SR2) \\\n",
        "            and not check_dots(word_img[:, SEG[0]:SEG[1]])):\n",
        "\n",
        "            # Case starts with س\n",
        "            if SEGP[0] != -1\\\n",
        "                and (check_stroke(no_dots_copy, no_dots_copy[:, SEGP[0]:SEGP[1]], upper_base, lower_base, SEGP_SR1, SEGP_SR2) \\\n",
        "                and not check_dots(word_img[:, SEGP[0]:SEGP[1]])):\n",
        "\n",
        "                SR_idx += 2\n",
        "                continue\n",
        "\n",
        "            # SEGN is stroke without dots\n",
        "            if SEGN[0] != -1 \\\n",
        "                and (check_stroke(no_dots_copy, no_dots_copy[:, SEGN[0]:SEGN[1]], upper_base, lower_base, SEGN_SR1, SEGN_SR2) \\\n",
        "                and not check_dots(word_img[:, SEGN[0]:SEGN[1]])):\n",
        "\n",
        "                valid_separation_regions.append(SR)\n",
        "                SR_idx += 3\n",
        "                continue\n",
        "\n",
        "            # SEGN stroke with Dots and SEGNN stroke without Dots\n",
        "            if SEGN[0] != -1\\\n",
        "                and (check_stroke(no_dots_copy, no_dots_copy[:, SEGN[0]:SEGN[1]], upper_base, lower_base, SEGN_SR1, SEGN_SR2) \\\n",
        "                and check_dots(word_img[:, SEGN[0]:SEGN[1]])) \\\n",
        "                and ((SEGNN[0] != -1 \\\n",
        "                and (check_stroke(no_dots_copy, no_dots_copy[:, SEGNN[0]:SEGNN[1]], upper_base, lower_base, SEGNN_SR1, SEGNN_SR2) \\\n",
        "                and not check_dots(word_img[:, SEGNN[0]:SEGNN[1]]))) or (len(SRL)-1-SR_idx == 2) or (len(SRL)-1-SR_idx == 3)):\n",
        "        \n",
        "                    valid_separation_regions.append(SR)\n",
        "                    SR_idx += 3\n",
        "                    continue\n",
        "            \n",
        "            # SEGN is not stroke or Stroke with Dots\n",
        "            if SEGN[0] != -1 \\\n",
        "                and ((not check_stroke(no_dots_copy, no_dots_copy[:, SEGN[0]:SEGN[1]], upper_base, lower_base, SEGN_SR1, SEGN_SR2)) \\\n",
        "                or (check_stroke(no_dots_copy, no_dots_copy[:, SEGN[0]:SEGN[1]], upper_base, lower_base, SEGN_SR1, SEGN_SR2) \\\n",
        "                and check_dots(word_img[:, SEGN[0]:SEGN[1]]))):\n",
        "                    \n",
        "                    SR_idx += 1\n",
        "                    continue\n",
        "            \n",
        "            SR_idx += 1\n",
        "            continue\n",
        "                \n",
        "\n",
        "        if (len(valid_separation_regions) == 0 or\\\n",
        "            len(valid_separation_regions) > 0 and abs(cut_idx-valid_separation_regions[-1][1]) > 2): \n",
        "            valid_separation_regions.append(SR)\n",
        "        SR_idx += 1\n",
        "\n",
        "    return valid_separation_regions\n",
        "\n",
        "\n",
        "def extract_char(img, valid_SR):\n",
        "\n",
        "    # binary image needs to be (0, 255) to be saved on disk not (0, 1)\n",
        "    img = img * 255\n",
        "    h, w = img.shape\n",
        "\n",
        "    next_cut = w\n",
        "    char_imgs = []\n",
        "\n",
        "    for SR in valid_SR:\n",
        "        char_imgs.append(img[:, SR[1]:next_cut])\n",
        "        next_cut = SR[1]\n",
        "    char_imgs.append(img[:, 0:next_cut])\n",
        "\n",
        "    return char_imgs\n",
        "\n",
        "\n",
        "def segment(line, word_img):\n",
        "\n",
        "    # binary_word = binarize(word_img)\n",
        "    binary_word = word_img//255\n",
        "    no_dots_copy = remove_dots(binary_word)\n",
        "\n",
        "    # l = binary_word.copy()\n",
        "\n",
        "    VP_no_dots = projection(no_dots_copy, 'vertical')\n",
        "    VP = projection(binary_word, 'vertical')\n",
        "    binary_word = fill(binary_word, VP_no_dots)\n",
        "    no_dots_copy = remove_dots(binary_word)\n",
        "\n",
        "    # sk = skeletonize(no_dots_copy)\n",
        "    upper_base, lower_base, MFV = baseline_detection(remove_dots(line))\n",
        "    MTI = horizontal_transitions(no_dots_copy, upper_base)\n",
        "        \n",
        "    SRL, wrong = cut_points(binary_word, VP, MFV, MTI, upper_base)\n",
        "\n",
        "    if wrong:\n",
        "        MTI -= 1\n",
        "        SRL.clear()\n",
        "        SRL, wrong = cut_points(binary_word, VP, MFV, MTI, upper_base)\n",
        "\n",
        "    HP = projection(line, 'horizontal')\n",
        "    top_line = -1\n",
        "\n",
        "    valid = filter_regions(binary_word, no_dots_copy, SRL, VP, upper_base, lower_base, MTI, MFV, top_line)\n",
        "\n",
        "    chars = extract_char(binary_word, valid)\n",
        "\n",
        "    return chars\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "    \n",
        "#     img = cv.imread('../Dataset/scanned/coct1183.png')\n",
        "#     lines = line_horizontal_projection(img)\n",
        "\n",
        "#     line = lines[0]\n",
        "#     words = word_vertical_projection(line)\n",
        "\n",
        "#     word = words[0]\n",
        "\n",
        "#     cr = segment(line, word)\n"
      ],
      "metadata": {
        "id": "UxHcJUthBScC"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "\n",
        "chars = ['ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف',\n",
        "'ق','ك', 'ل', 'م', 'ن', 'ه', 'و','ي','لا']\n",
        "train_ratio = 0.8\n",
        "script_path = os.getcwd()\n",
        "classifiers = [ svm.LinearSVC(),\n",
        "                MLPClassifier(alpha=1e-4, hidden_layer_sizes=(100,), max_iter=1000),\n",
        "                MLPClassifier(alpha=1e-5, hidden_layer_sizes=(200, 100,), max_iter=1000),\n",
        "                GaussianNB()]\n",
        "\n",
        "names = ['LinearSVM', '1L_NN', '2L_NN', 'Gaussian_Naive_Bayes']\n",
        "skip = [1, 0, 1, 1]\n",
        "\n",
        "width = 25\n",
        "height = 25\n",
        "dim = (width, height)\n",
        "\n",
        "\n",
        "def bound_box(img_char):\n",
        "    HP = projection(img_char, 'horizontal')\n",
        "    VP = projection(img_char, 'vertical')\n",
        "\n",
        "    top = -1\n",
        "    down = -1\n",
        "    left = -1\n",
        "    right = -1\n",
        "\n",
        "    i = 0\n",
        "    while i < len(HP):\n",
        "        if HP[i] != 0:\n",
        "            top = i\n",
        "            break\n",
        "        i += 1\n",
        "\n",
        "    i = len(HP)-1\n",
        "    while i >= 0:\n",
        "        if HP[i] != 0:\n",
        "            down = i\n",
        "            break\n",
        "        i -= 1\n",
        "\n",
        "    i = 0\n",
        "    while i < len(VP):\n",
        "        if VP[i] != 0:\n",
        "            left = i\n",
        "            break\n",
        "        i += 1\n",
        "\n",
        "    i = len(VP)-1\n",
        "    while i >= 0:\n",
        "        if VP[i] != 0:\n",
        "            right = i\n",
        "            break\n",
        "        i -= 1\n",
        "\n",
        "    return img_char[top:down+1, left:right+1]\n",
        "\n",
        "\n",
        "def binarize(char_img):\n",
        "    _, binary_img = cv.threshold(char_img, 127, 255, cv.THRESH_BINARY)\n",
        "    # _, binary_img = cv.threshold(word_img, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "    binary_char = binary_img // 255\n",
        "\n",
        "    return binary_char\n",
        "\n",
        "\n",
        "def prepare_char(char_img):\n",
        "\n",
        "    binary_char = binarize(char_img)\n",
        "\n",
        "    try:\n",
        "        char_box = bound_box(binary_char)\n",
        "        resized = cv.resize(char_box, dim, interpolation = cv.INTER_AREA)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return resized\n",
        "\n",
        "\n",
        "def featurizer(char_img):\n",
        "\n",
        "    flat_char = char_img.flatten()\n",
        "\n",
        "    return flat_char\n",
        "\n",
        "\n",
        "def read_data(limit=4000):\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    print(\"For each char\")\n",
        "    for char in tqdm(chars, total=len(chars)):\n",
        "\n",
        "        folder = f'/content/Arabic-OCR-master/Dataset/char_sample/{char}'\n",
        "        char_paths =  glob(f'/content/Arabic-OCR-master/Dataset/char_sample/{char}/*.png')\n",
        "\n",
        "        if os.path.exists(folder):\n",
        "            os.chdir(folder)\n",
        "\n",
        "            print(f'\\nReading images for char {char}')\n",
        "            for char_path in tqdm(char_paths[:limit], total=len(char_paths)):\n",
        "                num = re.findall(r'\\d+', char_path)[0]\n",
        "                char_img = cv.imread(f'{num}.png', 0)\n",
        "                ready_char = prepare_char(char_img)\n",
        "                feature_vector = featurizer(ready_char)\n",
        "                # X.append(char)\n",
        "                X.append(feature_vector)\n",
        "                Y.append(char)\n",
        "\n",
        "            os.chdir(script_path)\n",
        "            \n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def train():\n",
        "\n",
        "    X, Y = read_data()\n",
        "    assert(len(X) == len(Y))\n",
        "\n",
        "    X, Y = shuffle(X, Y)\n",
        "\n",
        "    X_train = []\n",
        "    Y_train = []\n",
        "    X_test = []\n",
        "    Y_test = []\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)\n",
        "    \n",
        "    X_train = np.array(X_train)\n",
        "    Y_train = np.array(Y_train)\n",
        "    X_test = np.array(X_test)\n",
        "    Y_test = np.array(Y_test)\n",
        "\n",
        "    scores = []\n",
        "    for idx, clf in tqdm(enumerate(classifiers), desc='Classifiers'):\n",
        "\n",
        "        if not skip[idx]:\n",
        "\n",
        "            clf.fit(X_train, Y_train)\n",
        "            score = clf.score(X_test, Y_test)\n",
        "            scores.append(score)\n",
        "            print(score)\n",
        "\n",
        "            # Save the model\n",
        "            destination = f'/content/Arabic-OCR-master/src/models'\n",
        "            if not os.path.exists(destination):\n",
        "                os.makedirs(destination)\n",
        "            \n",
        "            location = f'models/{names[idx]}.sav'\n",
        "            pickle.dump(clf, open(location, 'wb'))\n",
        "\n",
        "\n",
        "    with open('models/report.txt', 'w') as fo:\n",
        "        for score, name in zip(scores, names):\n",
        "            fo.writelines(f'Score of {name}: {score}\\n')\n",
        "\n",
        "\n",
        "def test(limit=3000):\n",
        "\n",
        "    location = f'models/{names[0]}.sav'\n",
        "    clf = pickle.load(open(location, 'rb'))\n",
        "     \n",
        "    X = []\n",
        "    Y = []\n",
        "    tot = 0\n",
        "    for char in tqdm(chars, total=len(chars)):\n",
        "\n",
        "        folder = f'/content/Arabic-OCR-master/Dataset/char_sample/{char}'\n",
        "        char_paths =  glob(f'/content/Arabic-OCR-master/Dataset/char_sample/{char}/*.png')\n",
        "\n",
        "\n",
        "        if os.path.exists(folder):\n",
        "            os.chdir(folder)\n",
        "\n",
        "            print(f'\\nReading images for char {char}')\n",
        "            tot += len(char_paths) - limit\n",
        "            for char_path in tqdm(char_paths[limit:], total=len(char_paths)):\n",
        "                num = re.findall(r'\\d+', char_path)[0]\n",
        "                char_img = cv.imread(f'{num}.png', 0)\n",
        "                ready_char = prepare_char(char_img)\n",
        "                feature_vector = featurizer(ready_char)\n",
        "                # X.append(char)\n",
        "                X.append(feature_vector)\n",
        "                Y.append(char)\n",
        "\n",
        "            os.chdir(script_path)\n",
        "    \n",
        "    cnt = 0\n",
        "    for x, y in zip(X, Y):\n",
        "\n",
        "        c = clf.predict([x])[0]\n",
        "        if c == y:\n",
        "            cnt += 1\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     train()\n",
        "#     # test()"
      ],
      "metadata": {
        "id": "5B5a-vcACFQH"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run2(obj):\n",
        "    word, line = obj\n",
        "    model = load_model()\n",
        "    # For each word in the image\n",
        "    char_imgs = segment(line, word)\n",
        "    txt_word = ''\n",
        "    # For each character in the word\n",
        "    for char_img in char_imgs:\n",
        "        try:\n",
        "            ready_char = prepare_char(char_img)\n",
        "        except:\n",
        "            # breakpoint()\n",
        "            continue\n",
        "        feature_vector = featurizer(ready_char)\n",
        "        predicted_char = model.predict([feature_vector])[0]\n",
        "        txt_word += predicted_char\n",
        "    return txt_word"
      ],
      "metadata": {
        "id": "43k4QRuU3K7T"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(image_path):\n",
        "    # Read test image\n",
        "    full_image = cv.imread(image_path)\n",
        "    predicted_text = ''\n",
        "\n",
        "    # Start Timer\n",
        "    before = time.time()\n",
        "    words = extract_words(full_image)       # [ (word, its line),(word, its line),..  ]\n",
        "    pool = mp.Pool(mp.cpu_count())\n",
        "    predicted_words = pool.map(run2, words)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    # Stop Timer\n",
        "    after = time.time()\n",
        "\n",
        "    # append in the total string.\n",
        "    for word in predicted_words:\n",
        "        predicted_text += word\n",
        "        predicted_text += ' '\n",
        "\n",
        "    exc_time = after-before\n",
        "    # Create file with the same name of the image\n",
        "    img_name = image_path.split('/')[-1].split('.')[0]\n",
        "\n",
        "    with open(f'output/text/{img_name}.txt', 'w', encoding='utf8') as fo:\n",
        "        fo.writelines(predicted_text)\n",
        "\n",
        "    return (img_name, exc_time)"
      ],
      "metadata": {
        "id": "DsI_50VP2_fZ"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('output'):\n",
        "    os.mkdir('output')\n",
        "open('output/running_time.txt', 'w').close()\n",
        "\n",
        "destination = 'output/text'\n",
        "if not os.path.exists(destination):\n",
        "    os.makedirs(destination)\n",
        "\n",
        "types = ['png', 'jpg', 'bmp']\n",
        "images_paths = []\n",
        "for t in types:\n",
        "    images_paths.extend(glob(f'/content/Arabic-OCR-master/src/test/*.{t}'))\n",
        "before = time.time()\n",
        "\n",
        "    # pool = mp.Pool(mp.cpu_count())\n",
        "\n",
        "    # # Method1\n",
        "    # for image_path in images_paths:\n",
        "    #     pool.apply_async(run,[image_path])\n",
        "\n",
        "    # Method2\n",
        "    # for _ in tqdm(pool.imap_unordered(run, images_paths), total=len(images_paths)):\n",
        "    #     pass\n",
        "\n",
        "running_time = []\n",
        "print(images_paths)\n",
        "for images_path in tqdm(images_paths,total=len(images_paths)):\n",
        "    running_time.append(run(images_path))\n",
        "\n",
        "running_time.sort()\n",
        "with open('output/running_time.txt', 'w') as r:\n",
        "    for t in running_time:\n",
        "        r.writelines(f'image#{t[0]}: {t[1]}\\n')       # if no need for printing 'image#id'.\n",
        "\n",
        "    # pool.close()\n",
        "    # pool.join()\n",
        "    after = time.time()\n",
        "    print(f'total time to finish {len(images_paths)} images:')\n",
        "    print(after - before)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSan4F7m0UXG",
        "outputId": "536977bb-83ca-406e-f8be-3bbf7b29a132"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/Arabic-OCR-master/src/test/capr12.png', '/content/Arabic-OCR-master/src/test/Diwani_Letter_2165.png', '/content/Arabic-OCR-master/src/test/capr14.jpg']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 1/3 [00:07<00:14,  7.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Arabic-OCR-master/src/test/capr12.png\n",
            "capr12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [00:08<00:03,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Arabic-OCR-master/src/test/Diwani_Letter_2165.png\n",
            "Diwani_Letter_2165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:11<00:00,  3.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Arabic-OCR-master/src/test/capr14.jpg\n",
            "capr14\n",
            "total time to finish 3 images:\n",
            "11.575642108917236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rQlD3PiQ2eh1"
      },
      "execution_count": 161,
      "outputs": []
    }
  ]
}